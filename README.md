# ğŸ¥ YouTube Video QnA Chat (RAG-powered with LangChain)

This project is an **interactive chatbot** that allows you to query any YouTube video using its transcript. It is powered by **RAG (Retrieval-Augmented Generation)** and **LangChain**, enabling the assistant to answer only from the videoâ€™s content.  

---

## ğŸš€ Features
- ğŸ” **Automatic Transcript Fetching** â€“ Extracts captions using [YouTube Transcript API](https://pypi.org/project/youtube-transcript-api/).  
- âœ‚ï¸ **Chunking & Embeddings** â€“ Transcript split into manageable chunks and embedded with `OpenAIEmbeddings`.  
- ğŸ§  **RAG Workflow** â€“ Questions are answered using retrieved chunks (not hallucinated).  
- ğŸ’¬ **Chat-based UI** â€“ Powered by **Streamlitâ€™s chat interface** with sequential history.  
- âš¡ **Streaming AI Responses** â€“ Answers appear **token by token** for a real-time feel.  
- âŒ **Exit Command** â€“ Type `exit` to reset and load a new YouTube video.  
- ğŸ¨ **Custom UI** â€“ Dark blue gradient background with white text for readability.  

---

## ğŸ› ï¸ How It Works (RAG + LangChain Behind the Scenes)

### ğŸ”¹ Step 1: Transcript Retrieval  
The transcript is fetched from YouTube using **YouTubeTranscriptApi**. If captions are disabled, the app stops gracefully.  

### ğŸ”¹ Step 2: Text Splitting  
The transcript (which can be thousands of words long) is divided into **overlapping chunks** (1000 characters, 200 overlap) using LangChainâ€™s `RecursiveCharacterTextSplitter`.  

### ğŸ”¹ Step 3: Embeddings & Vector Store  
- Each chunk is embedded into a **vector representation** using `OpenAIEmbeddings (text-embedding-3-small)`.  
- These embeddings are stored in **FAISS**, a fast similarity search library.  

### ğŸ”¹ Step 4: Retrieval  
When the user asks a question, the system:  
1. Embeds the query.  
2. Finds the **top-k (4)** most relevant transcript chunks using FAISS similarity search.  

### ğŸ”¹ Step 5: Prompt Construction  
A **LangChain PromptTemplate** ensures the model only uses transcript context:  

You are a helpful assistant.
Answer ONLY from the provided transcript context.
If the context is insufficient, just say you don't know.


### ğŸ”¹ Step 6: Answer Generation  
- Retrieved context + question â†’ sent to **ChatOpenAI (gpt-4o-mini)**.  
- Model streams its response back to the user, displayed in real time.  

### ğŸ”¹ Step 7: Stateful Chat  
- Chat history is stored in `st.session_state`.  
- If the user types `exit`, chat resets and prompts for a new video.  

---

ğŸ’» Tech Stack

    Frontend/UI: Streamlit
    LLM: OpenAI GPT-4o-mini
    RAG Components: LangChain
    Vector DB: FAISS
    Embeddings: OpenAI text-embedding-3-small
    Transcript API: youtube-transcript-api

ğŸ“Š Example Workflow

    User enters a YouTube video link â†’ App fetches transcript.
    Transcript is chunked & embedded â†’ stored in FAISS.
    User asks: â€œWhat was explained about machine learning in this video?â€
    FAISS retrieves top transcript chunks.
    LLM answers:
    "The speaker explained that machine learning allows systems to learn from data without explicit programming, highlighting supervised and unsupervised learning."

âš ï¸ Limitations

    âŒ Only works if video has captions available.
    âŒ Model wonâ€™t answer outside transcript scope (by design).
    ğŸ’° Requires an OpenAI API key (usage cost applies).


ğŸš€ Future Improvements

    Add multi-language support for transcripts.
    Store multiple videos in one session for cross-video QnA.
    Support offline embeddings (e.g., HuggingFace models).
    Add summarization mode (short summary of video).

## ğŸ“¸ App Screenshot  

![App Screenshot](./Screenshots/streamlit_app.png)
